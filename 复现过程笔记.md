# 一、原理详解

简单来说Eventpillars提取制作事件数据，体素网格，backbone（PFN层）提取特征，用了多层卷积长短期神经网络的变种（ConvLSTM）Adaptive ConvLSTM组成SRM和LRM，LRM处理prev_states储存了三个状态，SRM处理prev_features，储存了上一个状态的特征，共同储存时间维度的特征，RetinaNet作为检测头检测

重新梳理一遍网络结构，事件数据的处理为
![image](https://github.com/user-attachments/assets/30e2a57f-7285-4259-aa46-7723befb6c6f)



将事件数据的点云变为体素的结构，统计每个网格内事件数量，将这个特征和点云数据拼接起来

![image.png](DMANet+d1319969-d9f1-4a1c-8bd3-caace8b8aad4/image 1.png)



简单讲一下RetinaNet，R-CNN 和Fast R-CNN，这些方法使用两步流程，先预测边界框，然后使用回归对这些框中的物体进行分类。

YOLO直接将图像分为大网格，每个网格预测属于他的物体，直接输出概率和坐标。这意味着边界框和分类都发生在一个步骤中。这一步过程简化了操作并实现了实时效率，将检测变为回归问题。

RetinaNet采用的是和YOLO一样的单阶段检测，但是RetinaNet将类别和检测框放在了两个回归网络中，提出了foca_loss已解决类别不平衡的问题

# 二、环境准备

## pytorch环境

显卡版本

```Python
NVIDIA GeForce RTX 3060 Laptop GPU
专用 GPU 内存	0.9/6.0 GB
共享 GPU 内存	0.1/7.6 GB
GPU 内存	1.0/13.6 GB
```

安装包版本

torch                   2.7.0+cu126

torchaudio              2.7.0+cu126

torchvision             0.22.0+cu126

apex                    0.1（实际未用到）

Python                 3.9.21

cuda                      12.2

以及云端服务器使用的是在AuToDL租赁的单张NVIDIA 4090显卡，具体环境不再说明

## jittor环境

jittor                    1.3.7.0

Python                 3.9.21

cuda                    11.2(内置安装)

以及云端服务器使用的是在AuToDL租赁的单张NVIDIA 3090显卡，jittor为1.3.1.37，具体环境不再说明

# 三、数据集准备 

数据下载的是原论文已经准备好的数据，没有自己制作，原论文的数据有两百多G，由于设备资源有限，无法全部下载，挑选了部分（40G）的内容作为训练集

```Python
prophesee_dlut   
├── test
│   ├── testfilelist00

├── train
│   ├── trainfilelist00
    │   ├──events
    │   ├──labels
│   ├── trainfilelist01

└── val
    ├── valfilelist00
```

经查看发现每个.npz文件为十个时间步的事件集合

其中格式为：

lables为：(frame_id, x_center, y_center, width, height, class_id, confidence, timestamp)

events为：(timestamp, x, y, polarity)

**事件数据（Events）**：

- 每个事件数据会被存储在一个 `.npz` 文件中，包含多个时间步的事件数据。

- 每个事件数据将存储为一个名为 `e0, e1, ..., e9` 的数组，其中每个 `eX` 数组对应 10 个时间步的数据。

- `events[0]` 是一个 numpy 数组，形状为 `(N, 5)`，每行表示一个事件。事件的字段包括：

    - `t`: 事件的时间戳

    - `x`, `y`: 事件的位置（像素坐标）

    - `w`, `h`: 事件的大小（宽度和高度）

    - `class_id`: 事件的类别 ID

    - `class_confidence`: 类别的置信度

    - `track_id`: 跟踪 ID（标识目标对象）

**标签数据（Labels）**：

- 每个标签数据将存储在另一个 `.npz` 文件中，包含10个时间步的标签信息。

- 每个标签文件将包括多个命名为 `l0, l1, ..., l9` 的数组。

- 每个 `lX` 数组包含一个 `numpy` 数组，其中每行是一个检测框（bounding box）的信息：

    - `track_id`: 跟踪 ID

    - `class_id`: 类别 ID

    - `x`, `y`: 物体位置（坐标）

    - `w`, `h`: 物体的宽度和高度

    - `class_confidence`: 类别置信度

每个npz文件有十个通道，和传统的图像是别不同的是，事件数据的格式不是矩形的灰度值，而是一个包含N个事件的集合，每个元素有四个维度

体素化

|输出变量|含义|用途|
|-|-|-|
|`pos_voxels`|每个体素内的事件特征|模型输入特征|
|`pos_coordinates`|每个体素的位置|保留空间结构|
|`pos_num_points`|每个体素的点数量|用于 mask 或稀疏填充|

然后进行了裁剪，数据随机采样，标签打上batch_id等操作等操作

最终输入为

```Python
inputs
├── [0] 正样本 batch 列表（长度 = batch size）
│   ├── [0] 一个样本（tuple，含 6 个字段）
│   │   ├── [0] pos_pillar_x         → shape [1,1,N,5]
│   │   ├── [1] pos_pillar_y         → shape [1,1,N,5]
│   │   ├── [2] pos_pillar_t         → shape [1,1,N,5]
│   │   ├── [3] pos_num_points       → shape [1,N,]
│   │   ├── [4] pos_mask             → shape [1,1,N,5]
│   │   └── [5] pos_coors            → shape [N,3]
├── [1] 正样本 batch 列表（长度 = batch size）
│   ├── [0] 一个样本（tuple，含 6 个字段）
│   │   ├── [0] pos_pillar_x         → shape [1,1,N,5]
│   │   ├── [1] pos_pillar_y         → shape [1,1,N,5]
│   │   ├── [2] pos_pillar_t         → shape [1,1,N,5]
│   │   ├── [3] pos_num_points       → shape [1,N,]
│   │   ├── [4] pos_mask             → shape [1,1,N,5]
│   │   └── [5] pos_coors            → shape [N,3]
```

# 四、论文复现

## 1. 代码修改

首先关于pythorch版本的复现，按照起开源的网址进行复现，cuda版本为12.2，网上pythorch版本太老了，改为Python3.9适配的AMP就不能使用了，得用pytorch自带的混合精度训练方法

```Python
amp.register_float_function(torch, "sigmoid")
amp.register_float_function(torch, "softmax")
amp.register_float_function(torch, "matmul")
amp.register_float_function(torch.nn, "MaxPool2d")
```

告诉 `amp`：**即使开启混合精度训练，这些函数仍然强制使用 float32 精度执行（即不转换为 float16）**。

除此之外还有错误路径，重复拼接的问题的修复，函数用法改变等细节的修改不再展开

## 2.复现步骤

代码修改后就可以正常训练了，损失函数曲线如下

![image.png](DMANet+d1319969-d9f1-4a1c-8bd3-caace8b8aad4/image 2.png)

发现在十个epoch后出现了loss爆炸的现象，初步怀疑是梯度爆炸导致的，

做了如下检查：

1. 检查数据集标签是否错误

2. 检查代码是否出现除零

3. 检查数据是否归一化

发现数据标签以及代码都没用问题，采取以下措施重新训练

1. 降低了初始学习率（乘以0.1）

2. 降低了最小学习率（从0.1降到0.01）

3. 添加了权重衰减（weight_decay=1e-4）

4. 增加了梯度裁剪的阈值（从0.1增加到1.0）

发现loss曲线依旧出现了爆炸，无法正常收敛，不过延迟了很多，在20个epoch左右才开始无法收敛



![image.png](DMANet+d1319969-d9f1-4a1c-8bd3-caace8b8aad4/image 3.png)

仔细研究，还是怀疑梯度爆炸的原因，增加梯度裁剪，降低学习率等措施只能治标不治本。

测试一了下未爆炸前的模型文件表现

|Class|Events|Labels|Precision|Recall|mAP@0.5|mAP@0.5:0.95|
|-|-|-|-|-|-|-|
|**all**|3590|22099|0.851|0.283|0.311|0.203|
|pedestrian|3590|2164|0.651|0.0198|0.041|0.0194|
|two wheeler|3590|356|1.000|0.0409|0.0724|0.0407|
|car|3590|17426|0.892|0.582|0.627|0.376|
|truck|3590|1351|0.720|0.671|0.698|0.447|
|bus|3590|321|0.693|0.648|0.664|0.503|
|traffic sign|3590|72|1.000|0.000|0.0189|0.00378|
|traffic light|3590|409|1.000|0.0175|0.0588|0.0329|

发现是可以收敛的，但是测试集表现很差，仔细思考loss爆炸的原因后，最终发现只有在loss比较低的时候才会出现变为Nan值，这是对数损失（Log Loss）中常见的问题，模型预测的概率值非常接近0或1但预测错误，Loss值会急剧增大，而一旦增加成为nan值，就没办法收敛了，原论文在数据集非常大的情况下不会这么快出现过拟合的现象，所以基本还是数据集的原因。除此之外我觉得和原论文中的网络结构也有关系，LSTM模型可以有效缓解梯度爆炸的现象，但是由于论文中将LSTM结构变为convLSTM，引入了类似RNN的结构，破坏了原有的网络，所以更容易出现梯度爆炸。因为原始的数据集确实太大了设备原因没办法验证猜想是否正确，但是在后面进行了详细解释，在自己电脑上剔除了唯一的错误类别后就发现可以正常收敛。

总结为两点

1. 采用的损失函数为 Log loss，加上数据集较少，增加了过拟合的风险，导致模型过于自信而出现错误时，loss直接爆炸

2. 网络结构因为论文加上了一个相似度模块破坏了LSTM原有的结构，增加了梯度爆炸的概率

## 3. 问题理论推导

### Log loss

Log loss 是一种用于评估分类模型性能的指标，尤其适用于二分类问题。其核心思想是通过衡量预测概率与真实标签之间的差异来反映模型的准确性。公式为：

$\mathrm{Loss=-\frac{1}{N}\sum_{i=1}^Ny_i\cdot\log(p(y_i))+(1-y_i)\cdot\log(1-p(y_i))}$

其中，`y` 是真实标签（0 或 1），`p` 是模型预测的概率值。

原论文以RetinaNet网络的focal loss作为基础，而focal loss是二元交叉熵算是函数的变种，为了解决类别不平衡问题，但是其依旧是以log作为函数的

$\mathrm{FL}(p_\mathrm{t})=-(1-p_\mathrm{t})^\gamma\log(p_\mathrm{t}).$

所以当预测错误且概率接近极端值时，例如 `p` 接近 0 而 `y=1`，或 `p` 接近 1 而 `y=0`，对数项会趋向负无穷，导致 Loss 剧增，这就是复现过程中出现loss爆炸的根本原因，而这种现象是可以通过增加数据集、更好的网络结构等措施避免模型过于自信。

###  convLSTM

先从RNN说起，RNN结构如下

![image.png](DMANet+d1319969-d9f1-4a1c-8bd3-caace8b8aad4/image 4.png)

LSTM

![image.png](DMANet+d1319969-d9f1-4a1c-8bd3-caace8b8aad4/image 5.png)

对LSTM如何防止梯度爆炸详细解释要从RNN说起，RNN的统一定义为

$h_t=f\left(x_t,h_{t-1};\theta\right)$

其中$h_t$是每一步的输出，它由当前输入$x_t$和前一时刻输出$h_{t−1}$共同决定，而$θ$则是可训练参数。在做最基本的分析时，我们可以假设$h_t,x_t,θ$都是一维的，这可以让我们获得最直观的理解，并且其结果对高维情形仍有参考价值。之所以要考虑梯度，是因为我们目前主流的优化器还是梯度下降及其变种，因此要求我们定义的模型有一个比较合理的梯度。我们可以求得：

$\frac{dh_t}{d\theta}=\frac{\partial h_t}{\partial h_{t-1}}\frac{dh_{t-1}}{d\theta}+\frac{\partial h_t}{\partial\theta}$

所以在连续乘积的情况下，$\left|\frac{\partial h_{t}}{\partial h_{t-1}}\right|$大于一，则容易梯度爆炸，小于一则会梯度消失，也有可能有些时刻大于1，有些时刻小于1，最终稳定在1附近，但这样概率很小，需要很精巧地设计模型才行。

SimpleRNN的公式为：

$h_t=\tanh(Wx_t+Uh_{t-1}+b)$

对$h_{t-1}$求偏导

$\frac{\partial h_t}{\partial h_{t-1}}=\left(1-h_t^2\right)U$

显然，由于我们无法确定U的范围，因此$\left|\frac{\partial h_{t}}{\partial h_{t-1}}\right|$可能小于1也可能大于1，梯度消失/爆炸的风险是存在的。

那么LSTM的公式为：

$\begin{aligned}&\mathrm{f}_t=\sigma\left(W_fx_t+U_fh_{t-1}+b_f\right)\\&i_{t}=\sigma\left(W_ix_t+U_ih_{t-1}+b_i\right)\\&o_t=\sigma\left(W_ox_t+U_oh_{t-1}+b_o\right)\\&\hat{c}_{t}=\tanh(W_cx_t+U_ch_{t-1}+b_c)\\&c_{t}=f_t\circ c_{t-1}+i_t\circ\hat{c}_t\\&h_{t}=o_t\circ\tanh(c_t)\end{aligned}$

简单的从1维的情形来看，同样求偏导，则

$\frac{\partial c_t}{\partial c_{t-1}}=f_t+c_{t-1}\frac{\partial f_t}{\partial c_{t-1}}+\hat{c}_t\frac{\partial i_t}{\partial c_{t-1}}+i_t\frac{\partial\hat{c}_t}{\partial c_{t-1}}$

右端第一项$f_t$，也就是我们所说的“遗忘门”，从下面的论述我们可以知道一般情况下其余三项都是次要项，因此$f_t$是“主项”，由于$f_t$在0～1之间，因此就意味着梯度爆炸的风险将会很小，至于会不会梯度消失，取决于$f_t$是否接近于1。

关于次要项的证明，以第二项为例：

$\frac{\partial f_t}{\partial c_{t-1}} = f_t (1 - f_t) o_{t-1} \left(1 - \tanh^2 c_{t-1}\right) c_{t-1} U_f
$

注意到$f_t, 1 - f_t, o_{t-1},$ 都是在0~1之间, 也可以证明$\left|\left(1 - \tanh^2 c_{t-1}\right) c_{t-1}\right| < 0.45$, 因此它也在-1~1之间。所以$c_{t-1} \frac{\partial f_t}{\partial c_{t-1}}$就相当于1个$U_f$乘上4个门, 结果会变得更加小, 所以只要初始化不是很糟糕, 那么它都会被压缩得相当小, 因此占不到主导作用。跟简单RNN的梯度相比, 它多出了3个门, 大大限制了梯度的区间

剩下两项的结论也是类似的:

$\hat{c}_t \frac{\partial i_t}{\partial c_{t-1}} = i_t (1 - i_t) o_{t-1} \left(1 - \tanh^2 c_{t-1}\right) \hat{c}_t U_i
$

$i_t \frac{\partial \hat{c}_t}{\partial c_{t-1}} = \left(1 - \hat{c}_t^2\right) o_{t-1} \left(1 - \tanh^2 c_{t-1}\right) i_t U_c$

所以, 后面三项的梯度带有更多的“门”, 一般而言乘起来后会被压缩的更厉害, 因此占主导的项还是$f_t$, $f_t$在0~1之间这个特性决定了它梯度爆炸的风险很小, 同时$f_t$表明了模型对历史信息的依赖性, 也正好是历史梯度的保留程度, 两者相互自洽, 所以LSTM也能较好地缓解梯度消失问题。因此, LST同时较好地缓解了梯度消失/爆炸问题

以上基本全是抄录[https://kexue.fm/archives/7888](https://kexue.fm/archives/7888)的内容

---

那么为什么说论文中提到的convLSTM破坏了这种结构，论文提出的Adaptive ConvLSTM模块结构如下

![image.png](DMANet+d1319969-d9f1-4a1c-8bd3-caace8b8aad4/image 6.png)

而原始的ConvLSTM结构为

![image.png](DMANet+d1319969-d9f1-4a1c-8bd3-caace8b8aad4/image 7.png)

可以清除的看到，论文提到的变种多了一个自适应权重的分支，猜测问题就在这个分支上，这个分支是没有门控单元的，所以相当于引入了一个RNN结构进去，公式表达为

$\tilde{h}_{t-1}=h_{t-1}\odot W_t \\  \tilde{c}_{t-1}=c_{t-1}\odot W_t$

其中$C_t=f_t\odot C_{t-1}\oplus i_t\odot\tilde{C}_t$，代入

$C_t=(f_t\odot W_t)\odot C_{t-1}+i_t\odot\tilde{C_t}$

求偏导后就可以发现，$W_t$这一项是不被$f_t$完全控制的，这就是该结构容易导致梯度爆炸的原因，简单做了下实验，在较小的数据集下测试了将该模块改为普通的ConvLSTM模块观察表现

![image.png](DMANet+d1319969-d9f1-4a1c-8bd3-caace8b8aad4/image 8.png)

蓝色为ConvLSTM，红色则为Adaptive ConvLSTM，可以看到，虽然蓝色仍旧出现了梯度爆炸的现象（在最后一格），但是明显比红色的情况延缓了很多，并且出现的概率更小。

在测试集上表现为（只选取了一种类别）

Adaptive ConvLSTM

Class            Events              Labels           Precision           Recall             mAP@0.5           mAP@0.5:0.95 
 all                60               309                  1              0.773              0.778              0.572
 car                60               309                  1              0.773              0.778              0.572

ConvLSTM

Class            Events              Labels           Precision           Recall             mAP@0.5           mAP@0.5:0.95 
 all                60               309              0.979              0.903              0.907              0.647
 car                60               309              0.979              0.903              0.907              0.647

可以看到，使用简单的ConvLSTM反而有所提升，梯度爆炸也缓解了，猜想基本没问题

# 五、框架修改

## 1.环境设置主要区别

首先

1. 在 jittor 中，设备的选择（CPU/GPU）通常通过环境变量或者 jt.flags 设置，而不是像 PyTorch 那样用 torch.device 或 jt.device。

2. 在 jittor 中，数据加载和采样机制与 PyTorch 有所不同。
PyTorch 的 torch.utils.data.sampler.Sampler 主要用于自定义数据集的采样方式。而 jittor 的数据加载机制没有完全等价的 Sampler 类，

3. models/functions/warmup.py 主要内容是 PyTorch 的自定义学习率预热调度器 WarmUpLR，继承自 torch.optim.lr_scheduler._LRScheduler。jittor 没有完全等价的 _LRScheduler，所以改为了简单的学习率随批次迭代的方式

## 2、修改数据加载的部分

这部分的处理不需要修改

## 3、修改模型部分

### EmbedAggregator模块

在这个模块

```Python
self.embed_convs = nn.Sequential(nn.Conv2d(channels, channels, kernel_size, padding=(kernel_size-1)//2),
                                 nn.ReLU(),
                                 nn.Conv2d(channels, channels, kernel_size, padding=(kernel_size-1)//2),
                                 nn.ReLU())
```

注意这个地方

```Python
weights = jt.sum(curr_embed*prev_embed, dim=1, keepdims=True)
```

### eventpillars模块

首先关于cat操作，jittor的说明文档中没有提到，查阅后发现concat和torch中的cat相同

然后是zero等操作

### dmanet模块

该模块主要涉及跳过预训练模型的加载，jittor中没有该功能

### train_DMANet模块

训练过程

```Python
self.train_schedular=orch.optim.lr_scheduler.CosineAnnealingLR(）
```

改为

```Python
        self.train_schedular = ExponentialDecayLR(self.optimizer, decay_rate=self.settings.exponential_decay)
```

并创建了一个子类

jittor中没有

```Python
        self.train_sampler = torch.utils.data.sampler.SubsetRandomSampler(list(range(len(train_dataset))))
```

功能，创建一个JittorRandomSampler的类实现

在基本的地方修改完后，出现隐形的错误



```Python
                m.weight.data = jt.array(np.random.normal(0, math.sqrt(2. / n), m.weight.shape))
```

发现是由于失去混合精度计算，必须全部转换为float32不能是float64

基本的框架修改后就可以训练了，但是还有问题

```Python
bounding_box, pos_events, neg_events = sample_batched
```

最后发现是因为

```Python
            self.loader = dataset.set_attrs(
                batch_size=batch_size,
                num_workers=num_workers,
                drop_last=drop_last,
                shuffle=True,  # 训练时启用打乱
                buffer_size=dataset.buffer_size,
                collate_batch=collate_events
            )
```

直接使用jittor的dataset的方法，我尝试直接用jittor内置的dataset作为loader的输入

调试完成发现，一轮就要九个小时，而且locloss梯度爆炸了

后可以正常收敛了，然后发现cls也不对，一直没变过，经过我一点点的调查后发现，是jittor的反向传播语法也和pytorch不同，由于实在没钱了，只训练了两个epoch（记录的是每一step的损失函数变化）

![image.png](DMANet+d1319969-d9f1-4a1c-8bd3-caace8b8aad4/image 9.png)

对比pytorch的训练曲线（注意下标的区别，上图仅为前5k步的loss，下图为10k的曲线）

![image.png](DMANet+d1319969-d9f1-4a1c-8bd3-caace8b8aad4/image 10.png)





震荡比较明显，虽然有收敛趋势的，然后在小数据集上重新训练了30个批次，可以收敛



![image.png](DMANet+d1319969-d9f1-4a1c-8bd3-caace8b8aad4/image 11.png)

以及最终表现，可以看到和pytorch版本一致，

Class            Events              Labels           Precision           Recall             mAP@0.5           mAP@0.5:0.95 
 all                30               150              0.743              0.578              0.711              0.385
 car                30               150              0.743              0.578              0.711              0.385

以及最后的可视化结果


![image.png](DMANet+d1319969-d9f1-4a1c-8bd3-caace8b8aad4/image 12.png)

# 六、参考文献

[也来谈谈RNN的梯度消失/爆炸问题 - 科学空间|Scientific Spaces](https://kexue.fm/archives/7888)






# 七、jittor中比较隐秘的bug

## 1.jittor的数组相除（/）

发现targets = targets / torch.Tensor([[0.1, 0.1, 0.2, 0.2]])这个的原因

tensor([[ 0.0373,  0.1095, -0.5180,  0.2715],
        [ 0.0457,  0.0894, -0.3153,  0.0687],
        [ 0.0373, -0.0673, -0.5180,  0.2715],
        [ 0.0457, -0.0550, -0.3153,  0.0687],
        [ 0.0457, -0.1993, -0.3153,  0.0687],

经过除以[0.1,0.1,0.2,0.2]后正常应该为

tensor([[ 1.9855,  0.2340, -2.2487,  1.0621],
        [ 2.4318,  0.1910, -1.2350,  0.0484],
        [-1.5500,  0.2340, -2.2487,  1.0621],
        [-1.8984,  0.1910, -1.2350,  0.0484],
        [ 1.9855, -1.5338, -2.2487,  1.0621],
        [-1.5500, -1.5338, -2.2487,  1.0621],
        [-1.8984, -1.2523, -1.2350,  0.0484],
        [ 0.4869,  3.0666,  0.5219,  0.5590],
        [ 2.0618,  1.4917,  0.5219,  0.5590],

## 2.jittor.array在转换数组时

final_anchor_boxes_indexes_value = [i] * anchors_nms_idx.shape[0]
# final_anchor_boxes_indexes_value = jt.array([i] * anchors_nms_idx.shape[0])

jt.Var([          0           0  1636530664 -2012862720 -1811473856], dtype=int32)
变成乱码了，必须加上参数

final_anchor_boxes_indexes_value = jt.array([2, 2, 3, 3, 3], dtype=jt.int32)



## 3.code

i, ious = box_iou(predn[pi, :4], tbox[ti]).argmax(1)  # best ious, indices和python 正好反着来l



jt.array必须加上参数要不然会随机爆炸



4

找了两天最后发现是.contiguous()的问题，jittor新版本虽然有但是会导致出现负值

最后发现是因为迁移到云服务器上版本不一样，才意识到这个问题.contiguous()

